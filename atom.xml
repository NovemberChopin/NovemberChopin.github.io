<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JS-Blog</title>
  
  <subtitle>Man proposes, God disposes.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-10-26T07:48:12.434Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>JS</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Golang实现简单爬虫框架（5）——项目重构与数据存储</title>
    <link href="http://yoursite.com/2019/10/19/Golang%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%EF%BC%885%EF%BC%89/"/>
    <id>http://yoursite.com/2019/10/19/Golang实现简单爬虫框架（5）/</id>
    <published>2019-10-19T13:06:37.000Z</published>
    <updated>2019-10-26T07:48:12.434Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在上一篇文章<a href="https://www.jianshu.com/p/0eaaa1626d6f" target="_blank" rel="noopener">《Golang实现简单爬虫框架（4）——队列实现并发任务调度》</a>中，我们使用用队列实现了任务调度，接下来首先对两种并发方式做一个同构，使代码统一。然后添加数据存储模块。</p><p><strong>注意：本次并发是在上一篇文章简单并发实现的基础上修改，所以没有贴出全部代码，只是贴出部分修改部分，要查看完整项目代码，可以查看上篇文章，或者从github下载<a href="https://github.com/NovemberChopin/golang-crawler" target="_blank" rel="noopener">项目源代码查看</a></strong></p><h2 id="1、项目重构"><a href="#1、项目重构" class="headerlink" title="1、项目重构"></a>1、项目重构</h2><h3 id="（1）并发引擎"><a href="#（1）并发引擎" class="headerlink" title="（1）并发引擎"></a>（1）并发引擎</h3><p>通过分析我们发现，两种不同调度的区别是每个<code>worker</code>一个<code>channel</code>还是 所有<code>worker</code>共用一个<code>channel</code>，所以我们在接口中定义一个函数<code>WorkerChan()</code>，用来决定这件事，即<code>worker</code>一个<code>channel</code>还是 所有<code>worker</code>共用一个<code>channel</code>。此时<code>ConfigMasterWorkerChan</code>就不再需要了。</p><p>在项目文件concurrent.go中我们定义一个任务调度器Scheduler，如下：</p><pre><code class="go">// 任务调度器type Scheduler interface {    Submit(request Request) // 提交任务    ConfigMasterWorkerChan(chan Request)    WorkerReady(w chan Request)    Run()}</code></pre><p>但是在简单并发中我们只实现了<code>Submit</code>和<code>ConfigMasterWorkerChan</code>接口，而使用队列调度中却实现了接口的所有方法，所有我们同构一下使<code>concurrent.go</code>文件可以适用于两种不同的调度器。</p><p>因为在<code>createworker</code>函数中要使用<code>WorkerReady</code>函数，所以要传入一个<code>Scheduler</code>，但是这样显得比较重，我们可以利用接口组合，新建一个接口<code>ReadyNotifier</code>，这样在<code>createworker</code>函数中传入<code>ReadyNotifier</code>即可。</p><p>修改后的任务调度如下：</p><pre><code class="go">type Scheduler interface {    ReadyNotifier    Submit(request Request) // 提交任务    WorkerChan() chan Request    Run()}type ReadyNotifier interface {    WorkerReady(chan Request)}</code></pre><p>此时创建goroutine修改如下：</p><pre><code class="go">// 创建 goroutinefor i := 0; i &lt; e.WorkerCount; i++ {    //任务是每个 worker 一个 channel 还是 所有 worker 共用一个 channel 由WorkerChan 来决定    createWorker(e.Scheduler.WorkerChan(), out, e.Scheduler)}</code></pre><p>修改后的concurrent.go文件如下：</p><pre><code class="go">package engineimport (    &quot;log&quot;)// 并发引擎type ConcurrendEngine struct {    Scheduler   Scheduler    WorkerCount int}// 任务调度器type Scheduler interface {    ReadyNotifier    Submit(request Request) // 提交任务    WorkerChan() chan Request    Run()}type ReadyNotifier interface {    WorkerReady(chan Request)}func (e *ConcurrendEngine) Run(seeds ...Request) {    out := make(chan ParseResult)    e.Scheduler.Run()    // 创建 goruntine    for i := 0; i &lt; e.WorkerCount; i++ {        // 任务是每个 worker 一个 channel 还是 所有 worker 共用一个 channel 由WorkerChan 来决定        createWorker(e.Scheduler.WorkerChan(), out, e.Scheduler)    }    // engine把请求任务提交给 Scheduler    for _, request := range seeds {        e.Scheduler.Submit(request)    }    itemCount := 0    for {        // 接受 Worker 的解析结果        result := &lt;-out        for _, item := range result.Items {            log.Printf(&quot;Got item: #%d: %v\n&quot;, itemCount, item)            itemCount++        }        // 然后把 Worker 解析出的 Request 送给 Scheduler        for _, request := range result.Requests {            e.Scheduler.Submit(request)        }    }}func createWorker(in chan Request, out chan ParseResult, ready ReadyNotifier) {    go func() {        for {            ready.WorkerReady(in) // 告诉调度器任务空闲            request := &lt;-in            result, err := worker(request)            if err != nil {                continue            }            out &lt;- result        }    }()}</code></pre><h3 id="（2）简单并发调度器"><a href="#（2）简单并发调度器" class="headerlink" title="（2）简单并发调度器"></a>（2）简单并发调度器</h3><p>scheduler/simple.go</p><pre><code class="go">package schedulerimport &quot;crawler/engine&quot;type SimpleScheduler struct {    workerChan chan engine.Request}func (s *SimpleScheduler) WorkerChan() chan engine.Request {    // 此时所有 worker 共用同一个 channel，直接返回即可    return s.workerChan}func (s *SimpleScheduler) WorkerReady(w chan engine.Request) {}func (s *SimpleScheduler) Run() {    // 创建出 workchannel    s.workerChan = make(chan engine.Request)}func (s *SimpleScheduler) Submit(request engine.Request) {    // send request down to worker chan    go func() {        s.workerChan &lt;- request    }()}</code></pre><h3 id="（3）队列实现调度器"><a href="#（3）队列实现调度器" class="headerlink" title="（3）队列实现调度器"></a>（3）队列实现调度器</h3><p>scheduler/queued.go</p><p>添加<code>WorkerChan()</code>的实现即可</p><pre><code class="go">package schedulerimport &quot;crawler/engine&quot;// 使用队列来调度任务type QueuedScheduler struct {    requestChan chan engine.Request    workerChan  chan chan engine.Request}func (s *QueuedScheduler) WorkerChan() chan engine.Request {    // 对于队列实现来讲，每个 worker 共用一个 channel    return make(chan engine.Request)}// 提交请求任务到 requestChanfunc (s *QueuedScheduler) Submit(request engine.Request) {    s.requestChan &lt;- request}// 告诉外界有一个 worker 可以接收 requestfunc (s *QueuedScheduler) WorkerReady(w chan engine.Request) {    s.workerChan &lt;- w}func (s *QueuedScheduler) Run() {    s.workerChan = make(chan chan engine.Request)    s.requestChan = make(chan engine.Request)    go func() {        // 创建请求队列和工作队列        var requestQ []engine.Request        var workerQ []chan engine.Request        for {            var activeWorker chan engine.Request            var activeRequest engine.Request            if len(requestQ) &gt; 0 &amp;&amp; len(workerQ) &gt; 0 {                activeWorker = workerQ[0]                activeRequest = requestQ[0]            }            select {            case r := &lt;-s.requestChan: // 当 requestChan 收到数据                requestQ = append(requestQ, r)            case w := &lt;-s.workerChan: // 当 workerChan 收到数据                workerQ = append(workerQ, w)            case activeWorker &lt;- activeRequest: // 当请求队列和认读队列都不为空时，给任务队列分配任务                requestQ = requestQ[1:]                workerQ = workerQ[1:]            }        }    }()}</code></pre><h3 id="（4）main函数"><a href="#（4）main函数" class="headerlink" title="（4）main函数"></a>（4）main函数</h3><p>经过上述同构，在main函数中如需切换不同调度器，只需要相应的配置即可。</p><pre><code class="go">package mainimport (    &quot;crawler/engine&quot;    &quot;crawler/scheduler&quot;    &quot;crawler/zhenai/parser&quot;)func main() {    e := engine.ConcurrendEngine{        //Scheduler: &amp;scheduler.QueuedScheduler{},    // 队列实现调度器        Scheduler:   &amp;scheduler.SimpleScheduler{},    // 简单并发调度        WorkerCount: 50,    }    e.Run(engine.Request{        Url:       &quot;http://www.zhenai.com/zhenghun&quot;,        ParseFunc: parser.ParseCityList,    })}</code></pre><h2 id="2、数据存储"><a href="#2、数据存储" class="headerlink" title="2、数据存储"></a>2、数据存储</h2><h3 id="（1）Mgo的介绍安装"><a href="#（1）Mgo的介绍安装" class="headerlink" title="（1）Mgo的介绍安装"></a>（1）Mgo的介绍安装</h3><p>爬取到的数据不能仅仅在控制台打印出来，所以我们还要给爬虫添加数据存储模块。我们本次选择使用mongodb来存储我们的数据。</p><blockquote><p>mgo（音mango）是<a href="http://www.mongodb.org/" target="_blank" rel="noopener">MongoDB</a>的<a href="http://golang.org/" target="_blank" rel="noopener">Go语言</a>驱动，它用基于Go语法的简单API实现了丰富的特性，并经过良好测试。</p></blockquote><p>官方网址：<a href="http://labix.org/mgo" target="_blank" rel="noopener">http://labix.org/mgo</a></p><p>文档：<a href="https://gopkg.in/mgo.v2" target="_blank" rel="noopener">API docs for mgo</a></p><p>首先我们要安装mgo，打开终端，输入下面代码完成安装</p><pre><code class="go">go get gopkg.in/mgo.v2</code></pre><p>mgo基本操作都很简单，有数据库操作经验都可以很快上手。</p><h3 id="（2）爬虫引擎与数据格式"><a href="#（2）爬虫引擎与数据格式" class="headerlink" title="（2）爬虫引擎与数据格式"></a>（2）爬虫引擎与数据格式</h3><p>首先，爬虫引擎获取到数据要把数据发送给数据存储模块，而数据的传递用要用到<code>channel</code>，所以打开<code>concurrent.go</code>文件，在引擎添加<code>ItemChan</code>属性，如下所示：</p><p>爬取到数据需要把数据发送到数据存储模块，</p><pre><code class="go">package engine// 并发引擎type ConcurrendEngine struct {    Scheduler   Scheduler // 任务调度器    WorkerCount int       // 并发任务数量    ItemChan    chan Item // 数据保存 channel}// ...for {    // 接受 Worker 的解析结果    result := &lt;-out    for _, item := range result.Items {        // 当抓取一组数据后，进行保存        go func(item2 Item) {            e.ItemChan &lt;- item2        }(item)    }    // ...}// ...</code></pre><p>在<code>engine/types.go</code>中定义Item类型：</p><pre><code class="go">package engine// 请求结构type Request struct {    Url       string // 请求地址    ParseFunc func([]byte) ParseResult}// 解析结果结构type ParseResult struct {    Requests []Request // 解析出的请求    Items    []Item    // 解析出的内容}// 解析出的用户数据格式type Item struct {    Url     string      // 个人信息Url地址    Type    string      // table    Id      string      // Id    Payload interface{} // 详细信息}func NilParseFun([]byte) ParseResult {    return ParseResult{}}</code></pre><h3 id="（3）存储模块的实现"><a href="#（3）存储模块的实现" class="headerlink" title="（3）存储模块的实现"></a>（3）存储模块的实现</h3><p>在根目录下创建persist文件夹，然后创建itemsaver.go文件</p><pre><code class="go">// persist/itemsaver.gopackage persistimport (    &quot;context&quot;    &quot;crawler/engine&quot;    &quot;errors&quot;    &quot;gopkg.in/mgo.v2&quot;    &quot;gopkg.in/olivere/elastic.v5&quot;    &quot;log&quot;)func ItemSaver(index string) (chan engine.Item, error) {    // mongodb connect    session, err := mgo.Dial(&quot;localhost:27017&quot;)    if err != nil {        panic(err)    }    out := make(chan engine.Item)    go func() {        itemCount := 0        for {            // 接收到发送的 item            item := &lt;-out            log.Printf(&quot;Item Saver: got item #%d: %v\n&quot;,                itemCount, item)            itemCount++            // Save data in mongodb            err := mongo_save(session, index, item)            if err != nil {                // if have err, ignore it                log.Printf(&quot;Item Saver: error, saving item %v: %v&quot;,                    item, err)            }        }    }()    return out, nil}// 使用 MongoDB 保存数据func mongo_save(session *mgo.Session, dbName string, item engine.Item) error {    if item.Type == &quot;&quot; {        return errors.New(&quot;must supply Type&quot;)    }    c := session.DB(dbName).C(item.Type)    // 选择要操作的数据库与集合    err := c.Insert(item)        // 插入数据    if err != nil {        log.Fatal(err)    }    return nil}</code></pre><h3 id="（4）存储测试文件"><a href="#（4）存储测试文件" class="headerlink" title="（4）存储测试文件"></a>（4）存储测试文件</h3><p>我们把一条数据存入mongodb，然后再取出来，比对读出的数据和写入的数据是否相同</p><pre><code class="go">// persist/itemsaver_test.gppackage persistimport (    &quot;crawler/engine&quot;    &quot;crawler/model&quot;    &quot;encoding/json&quot;    &quot;fmt&quot;    &quot;gopkg.in/mgo.v2&quot;    &quot;gopkg.in/mgo.v2/bson&quot;    &quot;log&quot;    &quot;testing&quot;)func TestMongoSave(t *testing.T) {    // mongodb connect    session, err := mgo.Dial(&quot;localhost:27017&quot;)    if err != nil {        panic(err)    }    expected := engine.Item{        Url:  &quot;http://album.zhenai.com/u/1946858930&quot;,        Type: &quot;zhenai&quot;,        Id:   &quot;1946858930&quot;,        Payload: model.Profile{            Name:     &quot;為你垨候&quot;,            Gender:   &quot;女士&quot;,            Age:      40,            Height:   163,            Weight:   54,            Income:   &quot;5-8千&quot;,            Marriage: &quot;未婚&quot;,            Address:  &quot;佛山顺德区&quot;,        },    }    // 保存数据    err = mongo_save(session, &quot;crawler&quot;, expected)    if err != nil {        panic(err)    }    c := session.DB(&quot;crawler&quot;).C(&quot;zhenai&quot;)    var result engine.Item    // 查询数据    err = c.Find(bson.M{&quot;id&quot;: &quot;1946858930&quot;}).One(&amp;result)    // result 为 Json 类型    if err != nil {        log.Fatal(err)    }    fmt.Printf(&quot;%s, %s, %v\n&quot;, result.Url, result.Id, result.Payload)}</code></pre><h3 id="（5）parser模块"><a href="#（5）parser模块" class="headerlink" title="（5）parser模块"></a>（5）parser模块</h3><p>我们要在<code>parse/profile.go</code>文件中组装好需要保存到数据库的数据格式</p><pre><code class="go">// ...result := engine.ParseResult{    Items: []engine.Item{        {            Url:     url,            Type:    &quot;zhenai&quot;,            Id:      extractString([]byte(url), idUrlRe),            Payload: profile,        },    },}// ...</code></pre><h3 id="（6）main函数"><a href="#（6）main函数" class="headerlink" title="（6）main函数"></a>（6）main函数</h3><pre><code class="go">package mainimport (    &quot;crawler/engine&quot;    &quot;crawler/persist&quot;    &quot;crawler/scheduler&quot;    &quot;crawler/zhenai/parser&quot;)func main() {    itemChan, err := persist.ItemSaver()    if err != nil {        panic(err)    }    e := engine.ConcurrendEngine{        //Scheduler: &amp;scheduler.QueuedScheduler{},        Scheduler:   &amp;scheduler.SimpleScheduler{},        WorkerCount: 100,        ItemChan:    itemChan,    }    e.Run(engine.Request{        Url:       &quot;http://www.zhenai.com/zhenghun&quot;,        ParseFunc: parser.ParseCityList,    })}</code></pre><p>运行项目，打开mongodb可视化工具，可以看到爬取了54410条数据</p><p><img src="http://ps5qm062j.bkt.clouddn.com/%E6%89%B9%E6%B3%A8%202019-05-23%20182059.png" srcset="/img/loading.gif" alt=""></p><h2 id="3、总结"><a href="#3、总结" class="headerlink" title="3、总结"></a>3、总结</h2><p>我们首先把两种并发方式做一个同构，使代码统一，直接在main函数中使用不同的配置就可以切换调度器，简单方便。然后使用Mgo驱动操作数据，添加到mongodb中。内容有点多，很多代码没有完整的展示出来，希望大家可以下载<a href="https://github.com/NovemberChopin/golang-crawler" target="_blank" rel="noopener">项目源代码</a>，回滚到对应提交记录查看，效果会更好。 别无所求，只求随手给个star</p><p>下篇博客中我们会再当前博客的基础上添加数据展示功能</p><p>如果想获取<a href="https://coding.imooc.com/class/180.html" target="_blank" rel="noopener">Google工程师深度讲解go语言</a>视频资源的，可以在评论区留下邮箱。</p><p>如果觉得文章还可以，劳烦大人随手点个赞。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在上一篇文章&lt;a href=&quot;https://www.jianshu.com/p/0eaaa1626d6f&quot; target=&quot;_blank&quot;
      
    
    </summary>
    
      <category term="GO" scheme="http://yoursite.com/categories/GO/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="goroutine" scheme="http://yoursite.com/tags/goroutine/"/>
    
  </entry>
  
  <entry>
    <title>Golang实现简单爬虫框架（4）——队列实现并发任务调度</title>
    <link href="http://yoursite.com/2019/10/17/Golang%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%EF%BC%884%EF%BC%89/"/>
    <id>http://yoursite.com/2019/10/17/Golang实现简单爬虫框架（4）/</id>
    <published>2019-10-17T12:22:37.000Z</published>
    <updated>2019-10-26T07:46:30.840Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在上一篇文章<a href="https://www.jianshu.com/p/3ca93124017b" target="_blank" rel="noopener">《Golang实现简单爬虫框架（3）——简单并发版》</a>中我们实现了一个最简单并发爬虫，调度器为每一个<code>Request</code>创建一个<code>goroutine</code>，每个<code>goroutine</code>往<code>Worker</code>队列中分发任务，发完就结束。所有的<code>Worker</code>都在抢一个<code>channel</code>中的任务。但是这样做还是有些许不足之处，比如控制力弱：所有的Worker在抢同一个<code>channel</code>中的任务，我们没有办法控制给哪一个worker任务。</p><p>其实我们可以自己做一个任务分发的机制，我们来决定分发给哪一个Worker</p><p><strong>注意：本次并发是在上一篇文章简单并发实现的基础上修改，所以没有贴出全部代码，只是贴出部分修改部分，要查看完整项目代码，可以查看上篇文章，或者从github下载<a href="https://github.com/NovemberChopin/golang-crawler" target="_blank" rel="noopener">项目源代码查看</a></strong></p><h2 id="1、项目架构"><a href="#1、项目架构" class="headerlink" title="1、项目架构"></a>1、项目架构</h2><p>在上一篇文章实现简单并发的基础上，我们修改下<code>Scheduler</code>的任务分发机制</p><p><img src="http://ps5qm062j.bkt.clouddn.com/%E6%89%B9%E6%B3%A8%202019-05-29%20134103.png" srcset="/img/loading.gif" alt=""></p><ul><li>当<code>Scheduler</code>接收到一个<code>Request</code>后，不能直接发给<code>Worker</code>，也不能为每个<code>Request</code>创建一个<code>goroutine</code>，所以这里使用一个Request队列</li><li>同时我们想对<code>Worker</code>实现一个更多的控制，可以决定把任务分发给哪一个<code>Worker</code>，所以这里我们还需要一个<code>Worker</code>队列</li><li>当有了<code>Request</code>和<code>Worker</code>，我们就可以把选择的Request发送给选择的<code>Worker</code></li></ul><h2 id="2、队列实现任务调度器"><a href="#2、队列实现任务调度器" class="headerlink" title="2、队列实现任务调度器"></a>2、队列实现任务调度器</h2><p>在scheduler目录下创建queued.go文件</p><pre><code class="go">package schedulerimport &quot;crawler/engine&quot;// 使用队列来调度任务type QueuedScheduler struct {    requestChan chan engine.Request        // Request channel    // Worker channel, 其中每一个Worker是一个 chan engine.Request 类型    workerChan  chan chan engine.Request    }// 提交请求任务到 requestChannelfunc (s *QueuedScheduler) Submit(request engine.Request) {    s.requestChan &lt;- request}func (s *QueuedScheduler) ConfigMasterWorkerChan(chan engine.Request) {    panic(&quot;implement me&quot;)}// 告诉外界有一个 worker 可以接收 requestfunc (s *QueuedScheduler) WorkerReady(w chan engine.Request) {    s.workerChan &lt;- w}func (s *QueuedScheduler) Run() {    // 生成channel    s.workerChan = make(chan chan engine.Request)    s.requestChan = make(chan engine.Request)    go func() {        // 创建请求队列和工作队列        var requestQ []engine.Request        var workerQ []chan engine.Request        for {            var activeWorker chan engine.Request            var activeRequest engine.Request            // 当requestQ和workerQ同时有数据时            if len(requestQ) &gt; 0 &amp;&amp; len(workerQ) &gt; 0 {                activeWorker = workerQ[0]                activeRequest = requestQ[0]            }            select {            case r := &lt;-s.requestChan: // 当 requestChan 收到数据                requestQ = append(requestQ, r)            case w := &lt;-s.workerChan: // 当 workerChan 收到数据                workerQ = append(workerQ, w)            case activeWorker &lt;- activeRequest: // 当请求队列和认读队列都不为空时，给任务队列分配任务                requestQ = requestQ[1:]                workerQ = workerQ[1:]            }        }    }()}</code></pre><h2 id="3、爬虫引擎"><a href="#3、爬虫引擎" class="headerlink" title="3、爬虫引擎"></a>3、爬虫引擎</h2><p>修改后的concurrent.go文件如下</p><pre><code class="go">package engineimport (    &quot;log&quot;)// 并发引擎type ConcurrendEngine struct {    Scheduler   Scheduler    WorkerCount int}// 任务调度器type Scheduler interface {    Submit(request Request) // 提交任务    ConfigMasterWorkerChan(chan Request)    WorkerReady(w chan Request)    Run()}func (e *ConcurrendEngine) Run(seeds ...Request) {    out := make(chan ParseResult)    e.Scheduler.Run()    // 创建 goruntine    for i := 0; i &lt; e.WorkerCount; i++ {        createWorker(out, e.Scheduler)    }    // engine把请求任务提交给 Scheduler    for _, request := range seeds {        e.Scheduler.Submit(request)    }    itemCount := 0    for {        // 接受 Worker 的解析结果        result := &lt;-out        for _, item := range result.Items {            log.Printf(&quot;Got item: #%d: %v\n&quot;, itemCount, item)            itemCount++        }        // 然后把 Worker 解析出的 Request 送给 Scheduler        for _, request := range result.Requests {            e.Scheduler.Submit(request)        }    }}func createWorker(out chan ParseResult, s Scheduler) {    // 为每一个Worker创建一个channel    in := make(chan Request)    go func() {        for {            s.WorkerReady(in) // 告诉调度器任务空闲            request := &lt;-in            result, err := worker(request)            if err != nil {                continue            }            out &lt;- result        }    }()}</code></pre><h2 id="4、main函数"><a href="#4、main函数" class="headerlink" title="4、main函数"></a>4、main函数</h2><pre><code class="go">package mainimport (    &quot;crawler/engine&quot;    &quot;crawler/scheduler&quot;    &quot;crawler/zhenai/parser&quot;)func main() {    e := engine.ConcurrendEngine{        Scheduler:   &amp;scheduler.QueuedScheduler{},// 这里调用并发调度器        WorkerCount: 50,    }    e.Run(engine.Request{        Url:       &quot;http://www.zhenai.com/zhenghun&quot;,        ParseFunc: parser.ParseCityList,    })}</code></pre><p>运行结果如下：</p><p><img src="http://ps5qm062j.bkt.clouddn.com/%E6%89%B9%E6%B3%A8%202019-05-23%20100718.png" srcset="/img/loading.gif" alt=""></p><h2 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h2><p>在这篇文章中我们使用队列实现对并发任务的调度，从而实现了对Worker的控制。我们现在并发有两种实现方式，但是他们的调度方法是不同的，为了代码的统一，所以在下一篇文章中的内容有：</p><ul><li>对项目做一个同构</li><li>添加数据的存储模块。</li></ul><p>如果想获取<a href="https://coding.imooc.com/class/180.html" target="_blank" rel="noopener">Google工程师深度讲解go语言</a>视频资源的，可以在评论区留下邮箱。</p><p>项目的<a href="https://github.com/NovemberChopin/golang-crawler" target="_blank" rel="noopener">源代码</a>已经托管到Github上，对于各个版本都有记录，欢迎大家查看，记得给个star，在此先谢谢大家</p><p>如果觉得博客不错，劳烦大人给个赞，</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;在上一篇文章&lt;a href=&quot;https://www.jianshu.com/p/3ca93124017b&quot; target=&quot;_blank&quot;
      
    
    </summary>
    
      <category term="GO" scheme="http://yoursite.com/categories/GO/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="goroutine" scheme="http://yoursite.com/tags/goroutine/"/>
    
  </entry>
  
  <entry>
    <title>Golang实现简单爬虫框架（3）——简单并发版</title>
    <link href="http://yoursite.com/2019/10/15/Golang%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%EF%BC%883%EF%BC%89/"/>
    <id>http://yoursite.com/2019/10/15/Golang实现简单爬虫框架（3）/</id>
    <published>2019-10-15T12:22:37.000Z</published>
    <updated>2019-10-26T07:46:00.816Z</updated>
    
    <content type="html"><![CDATA[<p>在上篇文章<a href="https://www.jianshu.com/p/37abb663ac84" target="_blank" rel="noopener">Golang实现简单爬虫框架（2）——单任务版爬虫</a>中我们实现了一个简单的单任务版爬虫，对于单任务版爬虫，每次都要请求页面，然后解析数据，然后才能请求下一个页面。整个过程中，获取网页数据速度比较慢，那么我们就把获取数据模块做成并发执行。在项目的基础上，实现多任务并发版爬虫。</p><p>项目github地址：<a href="https://github.com/NovemberChopin/golang-crawler，" target="_blank" rel="noopener">https://github.com/NovemberChopin/golang-crawler，</a> 回滚到相应记录食用，效果更佳。</p><h2 id="1、项目架构"><a href="#1、项目架构" class="headerlink" title="1、项目架构"></a>1、项目架构</h2><p>首先我们把但任务版爬虫架构中的<code>Fetcher</code>模块和<code>Parser</code>模块合并成一个<code>Worker</code>模块，然后并发执行<code>Worker</code>模块</p><p><img src="http://ps5qm062j.bkt.clouddn.com/%E6%89%B9%E6%B3%A8%202019-05-22%20090404.png" srcset="/img/loading.gif" alt=""></p><p>然后得到并发版的架构图：</p><p><img src="http://ps5qm062j.bkt.clouddn.com/%E6%89%B9%E6%B3%A8%202019-05-22%20091229.png" srcset="/img/loading.gif" alt=""></p><ul><li><p>在并发版爬虫中，会同时执行多个<code>Worker</code>，每个<code>Worker</code>任务接受一个<code>Request</code>请求，然后请求页面解析数据，输出解析出的<code>Requests</code>和<code>Item</code></p></li><li><p>因为又很多<code>Request</code>和<code>Worker</code>，所以还需要<code>Scheduler</code>模块，负责对请求任务的调度处理</p></li><li><p><code>Engine</code>模块接受<code>Worker</code>发送的<code>Requests</code>和<code>Items</code>，当前我们先把<code>Items</code>打印出，把解析出的<code>Request</code>发送给调度器</p></li><li><p>其中<code>Engine</code>和<code>Scheduler</code>是一个<code>goroutine</code>，<code>Worker</code>包含多个<code>goroutine</code>，各个模块之间都是用<code>channel</code>进行连接</p><p>先放上重构后的项目文件结构：</p><p><img src="http://ps5qm062j.bkt.clouddn.com/%E6%89%B9%E6%B3%A8%202019-05-22%20093830.png" srcset="/img/loading.gif" alt="文件结构"></p></li></ul><h2 id="2、Worker实现"><a href="#2、Worker实现" class="headerlink" title="2、Worker实现"></a>2、Worker实现</h2><p>我们从engine.go中提取下面功能作为Worker模块，同时把engine.go 更名为simple.go。修改后的simple.go文件请自行调整，或者去github项目<a href="https://github.com/NovemberChopin/golang-crawler" target="_blank" rel="noopener">源代码</a>回滚查看。</p><p>engine/worker.go</p><pre><code class="go">package engineimport (    &quot;crawler/fetcher&quot;    &quot;log&quot;)// 输入 Request， 返回 ParseResultfunc worker(request Request) (ParseResult, error) {    log.Printf(&quot;Fetching %s\n&quot;, request.Url)    content, err := fetcher.Fetch(request.Url)    if err != nil {        log.Printf(&quot;Fetch error, Url: %s %v\n&quot;, request.Url, err)        return ParseResult{}, err    }    return request.ParseFunc(content), nil}</code></pre><p>对于每一个Worker接受一个请求，然后返回解析出的内容</p><h2 id="3、并发引擎Concurrent实现"><a href="#3、并发引擎Concurrent实现" class="headerlink" title="3、并发引擎Concurrent实现"></a>3、并发引擎Concurrent实现</h2><p>请大家根据架构图来看，效果会更好。</p><pre><code>package engineimport &quot;log&quot;// 并发引擎type ConcurrendEngine struct {   Scheduler   Scheduler    // 任务调度器   WorkerCount int            // 任务并发数量}// 任务调度器type Scheduler interface {   Submit(request Request) // 提交任务   ConfigMasterWorkerChan(chan Request)    // 配置初始请求任务}func (e *ConcurrendEngine) Run(seeds ...Request) {   in := make(chan Request)            // scheduler的输入   out := make(chan ParseResult)    // worker的输出   e.Scheduler.ConfigMasterWorkerChan(in)    // 把初始请求提交给scheduler   // 创建 goruntine   for i := 0; i &lt; e.WorkerCount; i++ {      createWorker(in, out)   }   // engine把请求任务提交给 Scheduler   for _, request := range seeds {      e.Scheduler.Submit(request)   }   itemCount := 0   for {      // 接受 Worker 的解析结果      result := &lt;-out      for _, item := range result.Items {         log.Printf(&quot;Got item: #%d: %v\n&quot;, itemCount, item)         itemCount++      }      // 然后把 Worker 解析出的 Request 送给 Scheduler      for _, request := range result.Requests {         e.Scheduler.Submit(request)      }   }}// 创建任务，调用worker，分发goroutinefunc createWorker(in chan Request, out chan ParseResult) {   go func() {      for {         request := &lt;-in         result, err := worker(request)         if err != nil {            continue         }         out &lt;- result      }   }()}</code></pre><h2 id="4、任务调度器Scheduler实现"><a href="#4、任务调度器Scheduler实现" class="headerlink" title="4、任务调度器Scheduler实现"></a>4、任务调度器Scheduler实现</h2><p>scheduler/scheduler.go</p><pre><code class="go">package schedulerimport &quot;crawler/engine&quot;type SimpleScheduler struct {    workerChan chan engine.Request}func (s *SimpleScheduler) Submit(request engine.Request) {    // send request down to worker chan    go func() {        s.workerChan &lt;- request    }()}// 把初始请求发送给 Schedulerfunc (s *SimpleScheduler) ConfigMasterWorkerChan(in chan engine.Request) {    s.workerChan = in}</code></pre><h2 id="5、main函数"><a href="#5、main函数" class="headerlink" title="5、main函数"></a>5、main函数</h2><pre><code class="go">package mainimport (    &quot;crawler/engine&quot;    &quot;crawler/scheduler&quot;    &quot;crawler/zhenai/parser&quot;)func main() {    e := engine.ConcurrendEngine{    // 配置爬虫引擎        Scheduler:   &amp;scheduler.SimpleScheduler{},        WorkerCount: 50,    }    e.Run(engine.Request{        // 配置爬虫目标信息        Url:       &quot;http://www.zhenai.com/zhenghun&quot;,        ParseFunc: parser.ParseCityList,    })}</code></pre><h2 id="6、小结"><a href="#6、小结" class="headerlink" title="6、小结"></a>6、小结</h2><p>本次博客我们实现一个最简单的并发版爬虫，调度器源源不断的接受任务，一旦有一个worker空闲，就给其分配任务。这样子有一个缺点，就是我们不知道我们分发出那么多worker的工作情况，对worker的控制力比较弱，所以在下次博客中会用队列来实现任务调度。</p><p>如果想获取<a href="https://coding.imooc.com/class/180.html" target="_blank" rel="noopener">Google工程师深度讲解go语言</a>视频资源的，可以在评论区留言。</p><p>项目的<a href="https://github.com/NovemberChopin/golang-crawler" target="_blank" rel="noopener">源代码</a>已经托管到Github上，对于各个版本都有记录，欢迎大家查看，记得给个star，在此先谢谢大家了</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上篇文章&lt;a href=&quot;https://www.jianshu.com/p/37abb663ac84&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Golang实现简单爬虫框架（2）——单任务版爬虫&lt;/a&gt;中我们实现了一个简单的单任务版爬虫，对于单任务
      
    
    </summary>
    
      <category term="GO" scheme="http://yoursite.com/categories/GO/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="goroutine" scheme="http://yoursite.com/tags/goroutine/"/>
    
  </entry>
  
  <entry>
    <title>Golang实现简单爬虫框架（2）——单任务版爬虫</title>
    <link href="http://yoursite.com/2019/10/12/Golang%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%EF%BC%882%EF%BC%89/"/>
    <id>http://yoursite.com/2019/10/12/Golang实现简单爬虫框架（2）/</id>
    <published>2019-10-12T18:41:26.000Z</published>
    <updated>2019-10-26T07:43:53.683Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇博客<a href="https://blog.csdn.net/november_chopin/article/details/90414805" target="_blank" rel="noopener">《Golang实现简单爬虫框架（1）——项目介绍与环境准备》</a>中我们介绍了go语言的开发环境搭建，以及爬虫项目介绍。<br><a id="more"></a><br>本次爬虫爬取的是<a href="http://www.zhenai.com" target="_blank" rel="noopener">珍爱网</a>的用户信息数据，爬取步骤为：</p><ul><li>1.进入珍爱网<a href="http://www.zhenai.com/zhenghun" target="_blank" rel="noopener">城市页面</a>爬取所有的城市信息</li><li>2.进入<a href="http://www.zhenai.com/zhenghun/yantai" target="_blank" rel="noopener">城市详情页</a>爬取用户URL地址信息</li><li>3.进入<a href="http://album.zhenai.com/u/1461901396" target="_blank" rel="noopener">用户详情页</a>爬取所需要的用户信息</li></ul><p><strong>注意：在本此爬虫项目中，只会实现一个简单的爬虫架构，包括单机版实现、简单并发版以及使用队列进行任务调度的并发版实现，以及数据存储和展示功能。不涉及模拟登录、动态IP等技术，如果你是GO语言新手想找练习项目或者对爬虫感兴趣的读者，请放心食用。</strong></p><h4 id="1、单任务版爬虫架构"><a href="#1、单任务版爬虫架构" class="headerlink" title="1、单任务版爬虫架构"></a>1、单任务版爬虫架构</h4><p>首先我们实现一个单任务版的爬虫，且不考虑数据存储与展示模块，首先把基本功能实现。下面是单任务版爬虫的整体框架</p><p><img src="http://ps5qm062j.bkt.clouddn.com/%E6%89%B9%E6%B3%A8%202019-05-20%20114847.png" srcset="/img/loading.gif" alt="单任务版爬虫整体框架"></p><p>下面是具体流程说明：</p><ul><li>1、首先需要配置种子请求，就是seed，存储项目爬虫的初始入口</li><li>2、把初始入口信息发送给爬虫引擎，引擎把其作为任务信息放入任务队列，只要任务队列不空就一直从任务队列中取任务</li><li>3、取出任务后，engine把要请求的任务交给Fetcher模块，Fetcher模块负责通过URL抓取网页数据，然后把数据返回给Engine</li><li>4、Engine收到网页数后，把数据交给解析（Parser）模块，Parser解析出需要的数据后返回给Engine，Engine收到解析出的信息在控制台打印出来</li></ul><p>项目目录</p><p><img src="http://ps5qm062j.bkt.clouddn.com/%E6%89%B9%E6%B3%A8%202019-05-20%20115906.png" srcset="/img/loading.gif" alt="项目目录"></p><h4 id="2、数据结构定义"><a href="#2、数据结构定义" class="headerlink" title="2、数据结构定义"></a>2、数据结构定义</h4><p>在正式开始讲解前先看一下项目中的数据结构。</p><pre><code class="go">// /engine/types.gopackage engine// 请求结构type Request struct {    Url       string // 请求地址    ParseFunc func([]byte) ParseResult    // 解析函数}// 解析结果结构type ParseResult struct {    Requests []Request     // 解析出的请求    Items    []interface{} // 解析出的内容}</code></pre><p><code>Request</code>表示一个爬取请求，包括请求的<code>URL</code>地址和使用的解析函数，其解析函数返回值是一个<code>ParseResult</code>类型，其中<code>ParseResult</code>类型包括解析出的请求和解析出的内容。解析内容<code>Items</code>是一个<code>interface{}</code>类型，即这部分具体数据结构由用户自己来定义。</p><p><strong><em>注意：对于<code>Request</code>中的解析函数，对于每一个URL使用城市列表解析器还是用户列表解析器，是由我们的具体业务来决定的，对于<code>Engine</code>模块不必知道解析函数具体是什么，只负责<code>Request</code>中的解析函数来解析传入的URL对应的网页数据</em></strong></p><p>需要爬取的数据的定义</p><pre><code class="go">// /model/profile.gopackage model// 用户的个人信息type Profile struct {    Name     string    Gender   string    Age      int    Height   int    Weight   int    Income   string    Marriage string    Address  string}</code></pre><h4 id="3、Fetcher的实现"><a href="#3、Fetcher的实现" class="headerlink" title="3、Fetcher的实现"></a>3、Fetcher的实现</h4><p>Fetcher模块任务是获取目标URL的网页数据，先放上代码。</p><pre><code class="go">// /fetcher/fetcher.gopackage fetcherimport (    &quot;bufio&quot;    &quot;fmt&quot;    &quot;io/ioutil&quot;    &quot;log&quot;    &quot;net/http&quot;    &quot;golang.org/x/net/html/charset&quot;    &quot;golang.org/x/text/encoding&quot;    &quot;golang.org/x/text/encoding/unicode&quot;    &quot;golang.org/x/text/transform&quot;)// 网页内容抓取函数func Fetch(url string) ([]byte, error) {    client := &amp;http.Client{}    req, err := http.NewRequest(&quot;GET&quot;, url, nil)    if err != nil {        log.Fatalln(err)    }    req.Header.Set(&quot;User-Agent&quot;, &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36&quot;)    resp, err := client.Do(req)    if err != nil {        return nil, err    }    defer resp.Body.Close()    // 出错处理    if resp.StatusCode != http.StatusOK {        return nil, fmt.Errorf(&quot;wrong state code: %d&quot;, resp.StatusCode)    }    // 把网页转为utf-8编码    bodyReader := bufio.NewReader(resp.Body)    e := determineEncoding(bodyReader)    utf8Reader := transform.NewReader(bodyReader, e.NewDecoder())    return ioutil.ReadAll(utf8Reader)}func determineEncoding(r *bufio.Reader) encoding.Encoding {    bytes, err := r.Peek(1024)    if err != nil {        log.Printf(&quot;Fetcher error %v\n&quot;, err)        return unicode.UTF8    }    e, _, _ := charset.DetermineEncoding(bytes, &quot;&quot;)    return e}</code></pre><p>因为许多网页的编码是GBK，我们需要把数据转化为utf-8编码，这里需要下载一个包来完成转换，打开终端输入<code>gopm get -g -v golang.org/x/text</code>可以把GBK编码转化为utf-8编码。在上面代码</p><pre><code class="go">bodyReader := bufio.NewReader(resp.Body)    e := determineEncoding(bodyReader)    utf8Reader := transform.NewReader(bodyReader, e.NewDecoder())</code></pre><p>可以写为<code>utf8Reader := transform.NewReader(resp.Body, simplifiedchinese.GBK.NewDecoder())</code>也是可以的。但是这样问题是通用性太差，我们怎么知道网页是不是GBK编码呢？此时还可以引入另外一个库，可以帮助我们判断网页的编码。打开终端输入<code>gopm get -g -v golang.org/x/net/html</code>。然后把判断网页编码模块提取为一个函数，如上代码所示。</p><h4 id="4、Parser模块实现"><a href="#4、Parser模块实现" class="headerlink" title="4、Parser模块实现"></a>4、Parser模块实现</h4><h5 id="1-解析城市列表与URL："><a href="#1-解析城市列表与URL：" class="headerlink" title="(1)解析城市列表与URL："></a>(1)解析城市列表与URL：</h5><pre><code class="go">// /zhenai/parser/citylist.gopackage parserimport (    &quot;crawler/engine&quot;    &quot;regexp&quot;)const cityListRe = `&lt;a href=&quot;(http://www.zhenai.com/zhenghun/[0-9a-z]+)&quot;[^&gt;]*&gt;([^&lt;]+)&lt;/a&gt;`// 解析城市列表func ParseCityList(bytes []byte) engine.ParseResult {    re := regexp.MustCompile(cityListRe)    // submatch 是 [][][]byte 类型数据    // 第一个[]表示匹配到多少条数据，第二个[]表示匹配的数据中要提取的任容    submatch := re.FindAllSubmatch(bytes, -1)    result := engine.ParseResult{}    //limit := 10    for _, item := range submatch {        result.Items = append(result.Items, &quot;City:&quot;+string(item[2]))        result.Requests = append(result.Requests, engine.Request{            Url:       string(item[1]),    // 每一个城市对应的URL            ParseFunc: ParseCity,        // 使用城市解析器        })        //limit--        //if limit == 0 {        //    break        //}    }    return result}</code></pre><p>在上述代码中，获取页面中所有的城市与URL，然后把每个城市的<code>URL</code>作为下一个<code>Request</code>的<code>URL</code>，对应的解析器是<code>ParseCity</code>城市解析器。</p><p>在对<code>ParseCityList</code>进行测试的时候，如果<code>ParseFunc: ParseCity,</code>,这样就会调用<code>ParseCity</code>函数，但是我们只想测试城市列表解析功能，不想调用<code>ParseCity</code>函数，此时可以定义一个函数<code>NilParseFun</code>,返回一个空的<code>ParseResult</code>，写成<code>ParseFunc: NilParseFun,</code>即可。</p><pre><code class="go">func NilParseFun([]byte) ParseResult {    return ParseResult{}}</code></pre><p>因为<code>http://www.zhenai.com/zhenghun</code>页面城市比较多，为了方便测试可以对解析的城市数量做一个限制，就是代码中的注释部分。</p><p><strong><em>注意：在解析模块，具体解析哪些信息，以及正则表达式如何书写，不是本次重点。重点是理解各个解析模块之间的联系与函数调用，同下</em></strong> </p><h5 id="2-解析用户列表与URL"><a href="#2-解析用户列表与URL" class="headerlink" title="(2)解析用户列表与URL"></a>(2)解析用户列表与URL</h5><pre><code class="go">// /zhenai/parse/city.gopackage parserimport (    &quot;crawler/engine&quot;    &quot;regexp&quot;)var cityRe = regexp.MustCompile(`&lt;a href=&quot;(http://album.zhenai.com/u/[0-9]+)&quot;[^&gt;]*&gt;([^&lt;]+)&lt;/a&gt;`)// 用户性别正则，因为在用户详情页没有性别信息，所以在用户性别在用户列表页面获取var sexRe = regexp.MustCompile(`&lt;td width=&quot;180&quot;&gt;&lt;span class=&quot;grayL&quot;&gt;性别：&lt;/span&gt;([^&lt;]+)&lt;/td&gt;`)// 城市页面用户解析器func ParseCity(bytes []byte) engine.ParseResult {    submatch := cityRe.FindAllSubmatch(bytes, -1)    gendermatch := sexRe.FindAllSubmatch(bytes, -1)    result := engine.ParseResult{}    for k, item := range submatch {        name := string(item[2])        gender := string(gendermatch[k][1])        result.Items = append(result.Items, &quot;User:&quot;+name)        result.Requests = append(result.Requests, engine.Request{            Url: string(item[1]),            ParseFunc: func(bytes []byte) engine.ParseResult {                return ParseProfile(bytes, name, gender)            },        })    }    return result}</code></pre><h5 id="3-解析用户数据"><a href="#3-解析用户数据" class="headerlink" title="(3)解析用户数据"></a>(3)解析用户数据</h5><pre><code class="go">package parserimport (    &quot;crawler/engine&quot;    &quot;crawler/model&quot;    &quot;regexp&quot;    &quot;strconv&quot;)var ageRe = regexp.MustCompile(`&lt;div class=&quot;m-btn purple&quot; [^&gt;]*&gt;([\d]+)岁&lt;/div&gt;`)var heightRe = regexp.MustCompile(`&lt;div class=&quot;m-btn purple&quot; [^&gt;]*&gt;([\d]+)cm&lt;/div&gt;`)var weightRe = regexp.MustCompile(`&lt;div class=&quot;m-btn purple&quot; [^&gt;]*&gt;([\d]+)kg&lt;/div&gt;`)var incomeRe = regexp.MustCompile(`&lt;div class=&quot;m-btn purple&quot; [^&gt;]*&gt;月收入:([^&lt;]+)&lt;/div&gt;`)var marriageRe = regexp.MustCompile(`&lt;div class=&quot;m-btn purple&quot; [^&gt;]*&gt;([^&lt;]+)&lt;/div&gt;`)var addressRe = regexp.MustCompile(`&lt;div class=&quot;m-btn purple&quot; [^&gt;]*&gt;工作地:([^&lt;]+)&lt;/div&gt;`)func ParseProfile(bytes []byte, name string, gender string) engine.ParseResult {    profile := model.Profile{}    profile.Name = name    profile.Gender = gender    if age, err := strconv.Atoi(extractString(bytes, ageRe)); err == nil {        profile.Age = age    }    if height, err := strconv.Atoi(extractString(bytes, heightRe)); err == nil {        profile.Height = height    }    if weight, err := strconv.Atoi(extractString(bytes, weightRe)); err == nil {        profile.Weight = weight    }    profile.Income = extractString(bytes, incomeRe)    profile.Marriage = extractString(bytes, marriageRe)    profile.Address = extractString(bytes, addressRe)    // 解析完用户信息后，没有请求任务    result := engine.ParseResult{        Items: []interface{}{profile},    }    return result}func extractString(contents []byte, re *regexp.Regexp) string {    submatch := re.FindSubmatch(contents)    if len(submatch) &gt;= 2 {        return string(submatch[1])    } else {        return &quot;&quot;    }}</code></pre><h4 id="5、Engine实现"><a href="#5、Engine实现" class="headerlink" title="5、Engine实现"></a>5、Engine实现</h4><p>Engine模块是整个系统的核心，获取网页数据、对数据进行解析以及维护任务队列。</p><pre><code class="go">// /engine/engine.gopackage engineimport (    &quot;crawler/fetcher&quot;    &quot;log&quot;)// 任务执行函数func Run(seeds ...Request) {    // 建立任务队列    var requests []Request    // 把传入的任务添加到任务队列    for _, r := range seeds {        requests = append(requests, r)    }    // 只要任务队列不为空就一直爬取    for len(requests) &gt; 0 {        request := requests[0]        requests = requests[1:]        // 抓取网页内容        log.Printf(&quot;Fetching %s\n&quot;, request.Url)        content, err := fetcher.Fetch(request.Url)        if err != nil {            log.Printf(&quot;Fetch error, Url: %s %v\n&quot;, request.Url, err)            continue        }        // 根据任务请求中的解析函数解析网页数据        parseResult := request.ParseFunc(content)        // 把解析出的请求添加到请求队列        requests = append(requests, parseResult.Requests...)        // 打印解析出的数据        for _, item := range parseResult.Items {            log.Printf(&quot;Got item %v\n&quot;, item)        }    }}</code></pre><p><code>Engine</code>模块主要是一个<code>Run</code>函数，接收一个或多个任务请求，首先把任务请求添加到任务队列，然后判断任务队列如果不为空就一直从队列中取任务，把任务请求的URL传给<code>Fetcher</code>模块得到网页数据，然后根据任务请求中的解析函数解析网页数据。然后把解析出的请求加入任务队列，把解析出的数据打印出来。</p><h4 id="6、main函数"><a href="#6、main函数" class="headerlink" title="6、main函数"></a>6、main函数</h4><pre><code class="go">package mainimport (    &quot;crawler/engine&quot;    &quot;crawler/zhenai/parser&quot;)func main() {    engine.Run(engine.Request{    // 配置请求信息即可        Url:       &quot;http://www.zhenai.com/zhenghun&quot;,        ParseFunc: parser.ParseCityList,    })}</code></pre><p>在<code>main</code>函数中直接调用<code>Run</code>方法，传入初始请求。</p><h4 id="7、总结"><a href="#7、总结" class="headerlink" title="7、总结"></a>7、总结</h4><p>本次博客中我们用Go语言实现了一个简单的单机版爬虫项目。仅仅聚焦与爬虫核心架构，没有太多复杂的知识，关键是理解<code>Engine</code>模块以及各个解析模块之间的调用关系。</p><p>缺点是单机版爬取速度太慢了，而且没有使用到go语言强大的并发特特性，所以我们下一章会在本次项目的基础上，重构项目为并发版的爬虫。</p><p>如果想获取<a href="https://coding.imooc.com/class/180.html" target="_blank" rel="noopener">Google工程师深度讲解go语言</a>视频资源的，可以在评论区留言。</p><p>项目的<a href="https://github.com/NovemberChopin/golang-crawler" target="_blank" rel="noopener">源代码</a>已经托管到Github上，对于各个版本都有记录，欢迎大家查看，记得给个star，在此先谢谢大家了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇博客&lt;a href=&quot;https://blog.csdn.net/november_chopin/article/details/90414805&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Golang实现简单爬虫框架（1）——项目介绍与环境准备》&lt;/a&gt;中我们介绍了go语言的开发环境搭建，以及爬虫项目介绍。&lt;br&gt;
    
    </summary>
    
      <category term="GO" scheme="http://yoursite.com/categories/GO/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Golang实现简单爬虫框架（1）——项目介绍与环境准备</title>
    <link href="http://yoursite.com/2019/10/10/Golang%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%EF%BC%881%EF%BC%89/"/>
    <id>http://yoursite.com/2019/10/10/Golang实现简单爬虫框架（1）/</id>
    <published>2019-10-10T13:30:47.000Z</published>
    <updated>2019-10-26T07:49:27.080Z</updated>
    
    <content type="html"><![CDATA[<p>最近学习Go语言，看了慕课网<a href="https://coding.imooc.com/class/180.html" target="_blank" rel="noopener">Google工程师深度讲解go语言</a>这门课，现在把课程中的爬虫项目整理出来，同时也作为自己学习的一个总结。本人菜鸟一枚，如有任何问题，欢迎大家指正。<br><a id="more"></a></p><h3 id="一、环境准备"><a href="#一、环境准备" class="headerlink" title="一、环境准备"></a>一、环境准备</h3><h4 id="1、go语言安装"><a href="#1、go语言安装" class="headerlink" title="1、go语言安装"></a>1、go语言安装</h4><p>Go安装包下载网址：<a href="https://studygolang.com/dl" target="_blank" rel="noopener">https://studygolang.com/dl</a></p><p>选择对应的版本下载即可</p><p><img src="http://ps5qm062j.bkt.clouddn.com/%E6%89%B9%E6%B3%A8%202019-05-21%20195940.png" srcset="/img/loading.gif" alt="GO语言安装包下载"></p><p>Windows用户推荐使用<strong>msi</strong>安装，简单方便，而且会自动配置好环境变量</p><p>打开Windows中的命令提示符（cmd.exe）执行命令：<code>go version</code>查看go语言版本</p><p><img src="http://ps5qm062j.bkt.clouddn.com/%E6%89%B9%E6%B3%A8%202019-05-21%20200437.png" srcset="/img/loading.gif" alt="查看GO语言版本"></p><h4 id="2、环境配置"><a href="#2、环境配置" class="headerlink" title="2、环境配置"></a>2、环境配置</h4><p>然后我们需要设置Go语言工作空间gopath目录(Go语言开发的项目路径)</p><p>Windows 设置如下，新建一个环境变量名称叫做GOPATH，值为你的工作目录，例如GOPATH=D:\Workspace</p><p>以上 %GOPATH% 目录约定有三个子目录：</p><p><strong>src 存放源代码（比如：.go .c .h .s等）</strong></p><p><strong>pkg 编译后生成的文件（比如：.a）</strong></p><p><strong>bin 编译后生成的可执行文件</strong></p><p>bin和pkg目录可以不创建，go命令会自动创建（如 go install），只需要创建src目录即可。</p><h4 id="3、Goland安装与破解"><a href="#3、Goland安装与破解" class="headerlink" title="3、Goland安装与破解"></a>3、Goland安装与破解</h4><h5 id="（1）安装"><a href="#（1）安装" class="headerlink" title="（1）安装"></a>（1）安装</h5><p>Goland官网：<a href="https://www.jetbrains.com/go/" target="_blank" rel="noopener">https://www.jetbrains.com/go/</a></p><p>选择对应版本下载安装即可</p><p><img src="http://ps5qm062j.bkt.clouddn.com/%E6%89%B9%E6%B3%A8%202019-05-21%20201226.png" srcset="/img/loading.gif" alt="Goland下载"></p><h5 id="（2）破解"><a href="#（2）破解" class="headerlink" title="（2）破解"></a>（2）破解</h5><p>关于Goland的破解方式网上已经又很多文章，可以参考这篇：<a href="https://blog.csdn.net/dodod2012/article/details/82589458" target="_blank" rel="noopener">https://blog.csdn.net/dodod2012/article/details/82589458</a></p><h3 id="二、项目介绍"><a href="#二、项目介绍" class="headerlink" title="二、项目介绍"></a>二、项目介绍</h3><p>本次爬虫爬取的是<a href="http://www.zhenai.com" target="_blank" rel="noopener">珍爱网</a>的用户信息数据，爬取步骤为：</p><ul><li>1.进入珍爱网<a href="http://www.zhenai.com/zhenghun" target="_blank" rel="noopener">城市页面</a>爬取所有的城市信息</li><li>2.进入<a href="http://www.zhenai.com/zhenghun/yantai" target="_blank" rel="noopener">城市详情页</a>爬取用户URL地址信息</li><li>3.进入<a href="http://album.zhenai.com/u/1461901396" target="_blank" rel="noopener">用户详情页</a>爬取所需要的用户信息</li></ul><p>爬虫算法如下</p><p><img src="D:\workplace\MarkDown\爬虫项目\images\批注 2019-05-20 111028.png" srcset="/img/loading.gif" alt="爬虫算法"></p><p>在下篇博客中，会实现一个单机版的爬虫项目，敬请关注。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近学习Go语言，看了慕课网&lt;a href=&quot;https://coding.imooc.com/class/180.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Google工程师深度讲解go语言&lt;/a&gt;这门课，现在把课程中的爬虫项目整理出来，同时也作为自己学习的一个总结。本人菜鸟一枚，如有任何问题，欢迎大家指正。&lt;br&gt;
    
    </summary>
    
      <category term="GO" scheme="http://yoursite.com/categories/GO/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="Goland" scheme="http://yoursite.com/tags/Goland/"/>
    
  </entry>
  
  <entry>
    <title>Slogan</title>
    <link href="http://yoursite.com/2018/04/23/Slogan/"/>
    <id>http://yoursite.com/2018/04/23/Slogan/</id>
    <published>2018-04-23T01:38:34.000Z</published>
    <updated>2018-04-23T16:36:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>Don’t aim for success if you really want it, just stick to what you love and believe in, and it will come natually!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Don’t aim for success if you really want it, just stick to what you love and believe in, and it will come natually!&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="English" scheme="http://yoursite.com/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>Hello Hexo</title>
    <link href="http://yoursite.com/2018/04/23/Hello-Hexo/"/>
    <id>http://yoursite.com/2018/04/23/Hello-Hexo/</id>
    <published>2018-04-23T01:21:34.000Z</published>
    <updated>2018-04-22T19:23:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>用Hexo+Github，妈妈再也不用担心我搭建博客了^_^</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;用Hexo+Github，妈妈再也不用担心我搭建博客了^_^&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
</feed>
